{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import html2text\n",
    "from datetime import date\n",
    "import random\n",
    "import os\n",
    "\n",
    "#Packages for pulling text data \n",
    "from urllib.request import urlopen  # the lib that handles the url stuff\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas.io.data as web\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "#Packages for text data processing\n",
    "import nltk, re, pprint\n",
    "#nltk.download()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#Packages for statistical learning \n",
    "from sklearn.preprocessing import normalize as Normal\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to get event listed in the raw 8-K text \n",
    "#Input:string (text) & string (events_list)\n",
    "#Output: pd.DataFrame\n",
    "def get_Events(text,events_list):\n",
    "    \n",
    "    events_stats=pd.DataFrame({\"count\":[0]*len(events_list)},index=events_list)\n",
    "    \n",
    "    for event in events_list:\n",
    "        try:\n",
    "            re.search(event,text)\n",
    "            if re.search(event,text):\n",
    "                events_stats.loc[event,\"count\"]=1 \n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return(events_stats)\n",
    "\n",
    "#Function to give the events list in a document  \n",
    "def Doc_Events(df,events_list):\n",
    "    ndocs=len(df)\n",
    "    nevents=len(events_list)\n",
    "    \n",
    "    df2=pd.DataFrame(np.random.randn(ndocs,nevents),index=df.index,columns=events_list)\n",
    "\n",
    "    for i in range(0,ndocs):\n",
    "        text=df.Text.iloc[i]\n",
    "        events_stats=get_Events(text,events_list)\n",
    "        df2.iloc[i]=events_stats[\"count\"]\n",
    "    \n",
    "    df3=pd.concat((df[['CIK','Company Name','Date Filed']],df2),axis=1)\n",
    "    \n",
    "    return(df3)\n",
    "    \n",
    "#Get aggregated events time series for the index \n",
    "def Index_Events(Events_df,Time_range):\n",
    "    \n",
    "    Temp=Events_df.sort_values(\"Date Filed\")\n",
    "    Temp=Temp.groupby(\"Date Filed\").sum()\n",
    "    Temp=Temp.drop(\"CIK\",axis=1)\n",
    "    \n",
    "    Time_temp=pd.DataFrame(index=Time_range)\n",
    "    Temp=Temp.merge(Time_temp,how=\"outer\",right_index=True,left_index=True)\n",
    "    Temp=Temp.fillna(0)\n",
    "    \n",
    "    return(Temp)\n",
    "\n",
    "#Get index of docs containing one event \n",
    "def get_event_doc(Events_df,item):\n",
    "    docs_index=Events_df[Events_df[item]==1].index\n",
    "    return(docs_index)\n",
    "\n",
    "#Get text containig one event \n",
    "def get_event_text(text,item):\n",
    "    beg_match=re.search(item,text)\n",
    "    beg=beg_match.start()\n",
    "    item_pattern=re.compile('ITEM\\s\\d.\\d\\d\\s\\s\\s+')\n",
    "    m=item_pattern.search(text,beg+1)\n",
    "    if m:\n",
    "        end=min(m.start(),len(text))\n",
    "    else: \n",
    "        end=len(text)\n",
    "    event_text=text[beg:end]\n",
    "    return(event_text)\n",
    "\n",
    "#Clean text Data \n",
    "def clean_text(raw_text,comp_name):\n",
    "\n",
    "    # 1. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_text) \n",
    "    comp_name=re.sub(\"[^a-zA-Z]\", \" \", comp_name) \n",
    "    \n",
    "    # 2. split into individual words\n",
    "    words = letters_only.lower().split()\n",
    "    comp_words=comp_name.lower().split()\n",
    "    \n",
    "    # 3. Convert stopwords to a list\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    \n",
    "    # 4. Remove stop words\n",
    "    meaningful_words = [w for w in words if not (w in stops or w in comp_words)]   \n",
    "    \n",
    "    # 5. Stemming to get root words \n",
    "    ps = PorterStemmer()\n",
    "    stemmed_words=[ps.stem(w) for w in meaningful_words]\n",
    "    \n",
    "    # return the result\n",
    "    return(\" \".join( stemmed_words ))  \n",
    "\n",
    "#Get noun of cleaned text data \n",
    "def get_noun(text):\n",
    "        \n",
    "    sentences = nltk.sent_tokenize(text) #tokenize sentences\n",
    "    nouns = [] #empty to array to hold all nouns\n",
    "\n",
    "    for sentence in sentences:\n",
    "         for word,pos in nltk.pos_tag(nltk.word_tokenize(str(sentence))):\n",
    "             #if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "             if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                 nouns.append(word)\n",
    "    return(\" \".join( nouns ))\n",
    "\n",
    "#Get top n keywords of a text \n",
    "def get_Keywords(s, n):\n",
    "    #s is a row of tfidf table\n",
    "    x=min(n, sum(s!=0)) #if there is less than 10 words in a line\n",
    "    temp = np.argpartition(-s, x) #get the top x keywords\n",
    "    top_n=s.iloc[temp[:x]].index.tolist()\n",
    "    top_n.extend([np.NaN]*(n-x))\n",
    "    return top_n\n",
    "\n",
    "#Get top n keywords of a text \n",
    "def Doc_Keywords(df, tfidf_df, n ):\n",
    "    df2=pd.DataFrame(index=df.index, columns=range(0,n))\n",
    "    for i, row in tfidf_df.iterrows():\n",
    "        #print(i)\n",
    "        df2.loc[i,:]=pd.Series(get_Keywords(row,n), index=range(0,n))\n",
    "    \n",
    "    df3=pd.concat((df[['CIK','Company Name','Date Filed']],df2),axis=1)\n",
    "    return df3\n",
    "\n",
    "#Define train data \n",
    "def get_train(X,Y,train_ratio):\n",
    "    train_sample=int(X.shape[0]*train_ratio)\n",
    "    X_train=X[:train_sample,:]\n",
    "    Y_train=Y[:train_sample]\n",
    "    return(X_train, Y_train )\n",
    "\n",
    "#Define test data \n",
    "def get_test(X,Y,train_ratio):\n",
    "    train_sample=int(X.shape[0]*train_ratio)\n",
    "    X_test=X[train_sample:,:]\n",
    "    Y_test=Y[train_sample:]\n",
    "    return(X_test, Y_test )\n",
    "\n",
    "#Get optimal lasso regression hyper-parameters \n",
    "def Lasso_param(X_train,Y_train):\n",
    "    \n",
    "    #rough grid\n",
    "    kf = KFold(n_splits=10)\n",
    "    parameters = {'alpha':np.arange(0,10,0.5)}\n",
    "    lasso = linear_model.Lasso()\n",
    "    clf = GridSearchCV(lasso, parameters,cv=kf)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    best_param=clf.best_params_['alpha']\n",
    "    print(best_param)\n",
    "    \n",
    "    #fine grid\n",
    "    parameters = {'alpha':np.arange(best_param-0.5,best_param+0.6,0.1)}\n",
    "    clf = GridSearchCV(lasso, parameters,cv=kf)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    best_param=clf.best_params_['alpha']\n",
    "\n",
    "    return best_param\n",
    "\n",
    "#Get optimal SVR regression hyper-parameters \n",
    "def SVR_param(X_train,Y_train):\n",
    "    \n",
    "    #rough grid\n",
    "    kf = KFold(n_splits=10)\n",
    "    parameters = {'C':np.exp(np.arange(-5,10,1))}\n",
    "    svr = SVR(kernel='linear',epsilon=0.01)\n",
    "    clf = GridSearchCV(svr, parameters,cv=kf)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    best_power=np.log(clf.best_params_['C'])\n",
    "    \n",
    "    #fine grid\n",
    "    parameters = {'C':np.exp(np.arange(best_power-0.5,best_power+0.6,0.1))}\n",
    "    clf = GridSearchCV(svr, parameters,cv=kf)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    best_power=np.log(clf.best_params_['C'])\n",
    "\n",
    "    return best_power\n",
    "\n",
    "#Get optimal SVR regression hyper-parameters  - to debug\n",
    "def SVM_param(X_train,Y_train):\n",
    "\n",
    "    #rough grid\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    parameters = {'C':np.exp(np.arange(-5,10,1))}\n",
    "    svc = svm.SVC(kernel='linear')\n",
    "    clf = GridSearchCV(svc, parameters,cv=skf)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    best_power=np.log(clf.best_params_['C'])\n",
    "    \n",
    "    #fine grid\n",
    "    parameters = {'C':np.exp(np.arange(best_power-0.5,best_power+0.6,0.1))}\n",
    "    clf = GridSearchCV(svc, parameters,cv=skf)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    best_power=np.log(clf.best_params_['C'])\n",
    "    \n",
    "    return(best_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KeywordAnalyzer:\n",
    "    \n",
    "    # constructor: pass in the keyword data frame, and the list of tickers associated with company names\n",
    "    def __init__(self, keywords_df, ticker_df ):\n",
    "        self.__Keywords=keywords_df.copy()\n",
    "        self.__Keywords['Ticker']=self.__Keywords['Company Name'].apply(lambda x:ticker_df[ticker_df.Name==x]['Ticker'].tolist()[0])\n",
    "        self.__Keywords['Ticker']=self.__Keywords['Ticker'].apply(lambda x:x.split(' ')[0])\n",
    "        try:\n",
    "            self.__Keywords['Date Filed']=self.__Keywords['Date Filed'].apply(lambda x: pd.to_datetime(x, format='%Y-%M-%d'))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        #initialize response dataframe\n",
    "        self.__Y=pd.DataFrame(index=keywords_df.index)\n",
    "        #initialize return time series\n",
    "        self.__TS=pd.DataFrame()\n",
    "        \n",
    "        \n",
    "    # display       \n",
    "    def get_X(self):\n",
    "        return self.__Keywords\n",
    "    \n",
    "    \n",
    "    # get Y data as response\n",
    "    def get_Y(self, kind='Return'):\n",
    "        if(kind in self.__Y.columns.tolist()): #already calculate\n",
    "            pass\n",
    "        else:\n",
    "            self.__Y[kind]=self.__Keywords.apply(lambda x: get_Ticker_Y(x.loc['Ticker'], x.loc['Date Filed'], kind),axis=1)\n",
    "            \n",
    "        return self.__Y\n",
    "            \n",
    "    # get stock return time series\n",
    "    def get_TS(self, window=10):\n",
    "        if(str(window-1) in self.__TS.columns.tolist()): #already called\n",
    "            pass\n",
    "        else:\n",
    "            #tmp_df=pd.DataFrame(index=self.__TS.index, columns=np.arange(window,-1,-1))\n",
    "            for i,row in self.__Keywords.iterrows():\n",
    "                tmp=get_Ticker_TS(row.Ticker, row['Date Filed'], window)\n",
    "                tmp.name=i\n",
    "                self.__TS=self.__TS.append(tmp)\n",
    "             \n",
    "        return (self.__TS)\n",
    "    \n",
    "    def prep_Classifier(self, kind='Return', normalize=True):\n",
    "        tmp=self.__Keywords.drop(['Ticker','CIK','Company Name','Date Filed'],axis=1).copy()\n",
    "        if(normalize):\n",
    "            TS_mat=Normal(self.__TS.as_matrix(),axis=0)\n",
    "            KW_mat=Normal(tmp.as_matrix(),axis=0)\n",
    "        else:\n",
    "            TS_mat=self.__TS.as_matrix()\n",
    "            KW_mat=tmp.as_matrix()\n",
    "                \n",
    "        label=(self.__Y[kind].as_matrix().T>0)*2-1\n",
    "        \n",
    "        return TS_mat, KW_mat, label\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper function\n",
    "def get_Ticker_Y(ticker, date, kind, window=22, window2=5):\n",
    "    #get trading day\n",
    "    if(date==date+BDay(0)): # if date is a trading day\n",
    "        T0=date\n",
    "        T1=date+BDay(1)\n",
    "    else:\n",
    "        T0=date-BDay(1)\n",
    "        T1=date+BDay(1)\n",
    "    \n",
    "    \n",
    "    if(kind=='Return'):# get T+1 return\n",
    "        try:\n",
    "            price = web.DataReader(ticker, 'yahoo', T0, T1)\n",
    "        except:\n",
    "            try:\n",
    "                price = web.DataReader(ticker, 'google', T0, T1)\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        try:\n",
    "            return price.Close[1]/price.Close[0]-1\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    elif(kind=='Return_Z'): # get T+1 Z score of return\n",
    "        try:\n",
    "            price = web.DataReader(ticker, 'yahoo', date-BDay(window), T1)\n",
    "        except:\n",
    "            try:\n",
    "                price = web.DataReader(ticker, 'google', date-BDay(window), T1)\n",
    "            except:\n",
    "                return 0\n",
    "        \n",
    "        daily_return=price.Close.pct_change(1)\n",
    "        return float((daily_return.tail(1)-daily_return.mean())/daily_return.std())\n",
    "        \n",
    "    elif(kind=='Vol_Ratio'): # get T+1 \n",
    "        try:\n",
    "            price = web.DataReader(ticker, 'yahoo', date-BDay(window), T1+BDay(window2-1))\n",
    "        except:\n",
    "            try:\n",
    "                price = web.DataReader(ticker, 'google', date-BDay(window), T1+BDay(window2-1))\n",
    "            except:\n",
    "                return 0\n",
    "            \n",
    "        daily_return=price.Close.pct_change(1)    \n",
    "        return float(daily_return.tail(5).std(ddof=0)/daily_return.head(22).std(ddof=0))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def get_Ticker_TS(ticker, date, window):\n",
    "    if(date==date+BDay(0)): # if date is a trading day\n",
    "        T0=date\n",
    "    else:\n",
    "        T0=date-BDay(1)\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        price = web.DataReader(ticker, 'yahoo', T0-BDay(window+3), T0)\n",
    "    except:\n",
    "        try:\n",
    "            price = web.DataReader(ticker, 'google', T0-BDay(window+3), T0)\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    r=price.Close.pct_change(1).tail(window)\n",
    "    r.index=['Lag '+ str(i) for i in np.arange(len(r)-1,-1,-1)]\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
